{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete TensorFlow mixed-precision implementation with Bert\n",
    "\n",
    "*It seems that mixed-precision isn't working in this kernel, but it does locally with suitable graphics card (try it out!), and reduces the runtime by a factor of 2.<br><br>*\n",
    "*Update 1: Corrected the implementation, so that it now works as it should.<br><br>*\n",
    "*Update 2: Small adjustments, and added hyperparameters to transformer model<br><br>*\n",
    "*Update 3: `\" \".join(set(selected.lower().split()))`<br><br>*\n",
    "*Update 4: `len(text[i].split()) < 2: decoded_text = text`<br><br>*\n",
    "*Update 5: Removing softmax step on predictions<br><br>*\n",
    "\n",
    "Done in three steps:\n",
    "1. define preprocessing procedure and add it to tf.data.Dataset.from_generator\n",
    "2. define Bert model together with train, predict and decode_prediction functions\n",
    "3. run the K-fold cross-validation including everything defined in step 1. and 2.\n",
    "\n",
    "Note: A simple **post processing** is used: predicting all 'neutral' sentiments to full text.\n",
    "\n",
    "credits to [abhishek](https://www.kaggle.com/abhishek) and all the 'get started' kernels to help/inspire me and many others to get started with this competition. The preprocessing in this notebook follows abhishek's [implementation](https://www.kaggle.com/abhishek/tweet-text-extraction-roberta-infer) with some modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil, floor\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from sklearn import model_selection\n",
    "from transformers import BertConfig, TFBertPreTrainedModel, TFBertMainLayer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "tf.config.optimizer.set_jit(True)\n",
    "tf.config.optimizer.set_experimental_options(\n",
    "    {\"auto_mixed_precision\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape = (27485, 4)\n",
      "test shape  = (3535, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a3d0a7d5ad</td>\n",
       "      <td>Spent the entire morning in a meeting w/ a ven...</td>\n",
       "      <td>my boss was not happy w/ them. Lots of fun.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>251b6a6766</td>\n",
       "      <td>Oh! Good idea about putting them on ice cream</td>\n",
       "      <td>Good</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c9e8d1ef1c</td>\n",
       "      <td>says good (or should i say bad?) afternoon!  h...</td>\n",
       "      <td>says good (or should i say bad?) afternoon!</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f14f087215</td>\n",
       "      <td>i dont think you can vote anymore! i tried</td>\n",
       "      <td>i dont think you can vote anymore!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bf7473b12d</td>\n",
       "      <td>haha better drunken tweeting you mean?</td>\n",
       "      <td>better</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1915bebcb3</td>\n",
       "      <td>headache  wanna see my Julie</td>\n",
       "      <td>headache</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2ab82634d5</td>\n",
       "      <td>had an awsome salad! I recommend getting the S...</td>\n",
       "      <td>had an awsome salad!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a5a1c996c0</td>\n",
       "      <td>fine! Going to do my big walk today 20 or so ...</td>\n",
       "      <td>fine!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a182b2638e</td>\n",
       "      <td>Thank a yoou  how are you? #TwitterTakeover</td>\n",
       "      <td>Thank</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1dcb6fdb13</td>\n",
       "      <td>Why don't adobe realise no one WANTS to pay fo...</td>\n",
       "      <td>Why don't adobe realise no one WANTS to pay fo...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  a3d0a7d5ad  Spent the entire morning in a meeting w/ a ven...   \n",
       "1  251b6a6766      Oh! Good idea about putting them on ice cream   \n",
       "2  c9e8d1ef1c  says good (or should i say bad?) afternoon!  h...   \n",
       "3  f14f087215         i dont think you can vote anymore! i tried   \n",
       "4  bf7473b12d             haha better drunken tweeting you mean?   \n",
       "5  1915bebcb3                       headache  wanna see my Julie   \n",
       "6  2ab82634d5  had an awsome salad! I recommend getting the S...   \n",
       "7  a5a1c996c0   fine! Going to do my big walk today 20 or so ...   \n",
       "8  a182b2638e        Thank a yoou  how are you? #TwitterTakeover   \n",
       "9  1dcb6fdb13  Why don't adobe realise no one WANTS to pay fo...   \n",
       "\n",
       "                                       selected_text sentiment  \n",
       "0        my boss was not happy w/ them. Lots of fun.   neutral  \n",
       "1                                               Good  positive  \n",
       "2        says good (or should i say bad?) afternoon!   neutral  \n",
       "3                 i dont think you can vote anymore!  negative  \n",
       "4                                             better  positive  \n",
       "5                                           headache  negative  \n",
       "6                               had an awsome salad!  positive  \n",
       "7                                              fine!  positive  \n",
       "8                                              Thank  positive  \n",
       "9  Why don't adobe realise no one WANTS to pay fo...   neutral  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv files\n",
    "train_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n",
    "train_df.dropna(inplace=True)\n",
    "\n",
    "test_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "test_df.loc[:, \"selected_text\"] = test_df.text.values\n",
    "\n",
    "submission_df = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
    "\n",
    "print(\"train shape =\", train_df.shape)\n",
    "print(\"test shape  =\", test_df.shape)\n",
    "\n",
    "# set some global variables\n",
    "PATH = \"../input/bert-base-uncased/\"\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "TOKENIZER = BertWordPieceTokenizer(f\"{PATH}/vocab.txt\", lowercase=True)\n",
    "\n",
    "# let's take a look at the data\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "I. Set up preprocessing and dataset/datagenerator\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(tweet, selected_text, sentiment):\n",
    "    \"\"\"\n",
    "    Will be used in tf.data.Dataset.from_generator(...)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # The original strings have been converted to \n",
    "    # byte strings, so we need to decode it\n",
    "    tweet = tweet.decode('utf-8')\n",
    "    selected_text = selected_text.decode('utf-8')\n",
    "    sentiment = sentiment.decode('utf-8')\n",
    "    \n",
    "    # Clean up the strings a bit\n",
    "    tweet = \" \".join(str(tweet).split())\n",
    "    selected_text = \" \".join(str(selected_text).split())\n",
    "    \n",
    "    # find the intersection between text and selected text\n",
    "    idx_start, idx_end = None, None\n",
    "    for index in (i for i, c in enumerate(tweet) if c == selected_text[0]):\n",
    "        if tweet[index:index+len(selected_text)] == selected_text:\n",
    "            idx_start = index\n",
    "            idx_end = index + len(selected_text)\n",
    "            break\n",
    "    \n",
    "    intersection = [0] * len(tweet)\n",
    "    if idx_start != None and idx_end != None:\n",
    "        for char_idx in range(idx_start, idx_end):\n",
    "            intersection[char_idx] = 1\n",
    "    \n",
    "    # tokenize with offsets\n",
    "    enc = TOKENIZER.encode(tweet)\n",
    "    input_ids_orig, offsets = enc.ids, enc.offsets\n",
    "\n",
    "    # compute targets\n",
    "    target_idx = []\n",
    "    for i, (o1, o2) in enumerate(offsets):\n",
    "        if sum(intersection[o1: o2]) > 0:\n",
    "            target_idx.append(i)\n",
    "    \n",
    "    target_start = target_idx[0]\n",
    "    target_end = target_idx[-1]\n",
    "\n",
    "    # add and pad data (hardcoded for BERT)\n",
    "    # --> [CLS] sentiment [SEP] input_ids [SEP] [PAD]\n",
    "    sentiment_map = {\n",
    "        'positive': 3893,\n",
    "        'negative': 4997,\n",
    "        'neutral': 8699,\n",
    "    }\n",
    "    \n",
    "    input_ids = [101] + [sentiment_map[sentiment]] + [102] + input_ids_orig + [102]\n",
    "    input_type_ids = [0] * (len(input_ids_orig) + 4)\n",
    "    attention_mask = [1] * (len(input_ids_orig) + 4)\n",
    "    offsets = [(0, 0), (0, 0), (0, 0)] + offsets + [(0, 0)]\n",
    "    target_start += 3\n",
    "    target_end += 3\n",
    "\n",
    "    padding_length = MAX_SEQUENCE_LENGTH - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([0] * padding_length)\n",
    "        attention_mask = attention_mask + ([0] * padding_length)\n",
    "        input_type_ids = input_type_ids + ([0] * padding_length)\n",
    "        offsets = offsets + ([(0, 0)] * padding_length)\n",
    "    elif padding_length < 0:\n",
    "        # not yet implemented\n",
    "        # truncates if input length > max_seq_len\n",
    "        pass\n",
    "        \n",
    "    return (\n",
    "        input_ids, attention_mask, input_type_ids, offsets,\n",
    "        target_start, target_end, tweet, selected_text, sentiment, \n",
    "    )\n",
    "\n",
    "\n",
    "class TweetSentimentDataset(tf.data.Dataset):\n",
    "    \n",
    "    OUTPUT_TYPES = (\n",
    "        tf.dtypes.int32,  tf.dtypes.int32,   tf.dtypes.int32, \n",
    "        tf.dtypes.int32,  tf.dtypes.float32, tf.dtypes.float32,\n",
    "        tf.dtypes.string, tf.dtypes.string,  tf.dtypes.string,\n",
    "    )\n",
    "    \n",
    "    OUTPUT_SHAPES = (\n",
    "        (128,),   (128,), (128,), \n",
    "        (128, 2), (),     (),\n",
    "        (),       (),     (),\n",
    "    )\n",
    "    \n",
    "    # AutoGraph will automatically convert Python code to\n",
    "    # Tensorflow graph code. You could also wrap 'preprocess' \n",
    "    # in tf.py_function(..) for arbitrary python code\n",
    "    def _generator(tweet, selected_text, sentiment):\n",
    "        for tw, st, se in zip(tweet, selected_text, sentiment):\n",
    "            yield preprocess(tw, st, se)\n",
    "    \n",
    "    # This dataset object will return a generator\n",
    "    def __new__(cls, tweet, selected_text, sentiment):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_types=cls.OUTPUT_TYPES,\n",
    "            output_shapes=cls.OUTPUT_SHAPES,\n",
    "            args=(tweet, selected_text, sentiment)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def create(dataframe, batch_size, shuffle_buffer_size=-1):\n",
    "        dataset = TweetSentimentDataset(\n",
    "            dataframe.text.values, \n",
    "            dataframe.selected_text.values, \n",
    "            dataframe.sentiment.values\n",
    "        )\n",
    "\n",
    "        dataset = dataset.cache()\n",
    "        if shuffle_buffer_size != -1:\n",
    "            dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        return dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "II. Set up transformer model and functions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BertQAModel(TFBertPreTrainedModel):\n",
    "    \n",
    "    DROPOUT_RATE = 0.1\n",
    "    NUM_HIDDEN_STATES = 2\n",
    "    \n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        \n",
    "        self.bert = TFBertMainLayer(config, name=\"bert\")\n",
    "        self.concat = L.Concatenate()\n",
    "        self.dropout = L.Dropout(self.DROPOUT_RATE)\n",
    "        self.qa_outputs = L.Dense(\n",
    "            config.num_labels, \n",
    "            kernel_initializer=TruncatedNormal(stddev=config.initializer_range),\n",
    "            dtype='float32',\n",
    "            name=\"qa_outputs\")\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # outputs: Tuple[sequence, pooled, hidden_states]\n",
    "        _, _, hidden_states = self.bert(inputs, **kwargs)\n",
    "        \n",
    "        hidden_states = self.concat([\n",
    "            hidden_states[-i] for i in range(1, self.NUM_HIDDEN_STATES+1)\n",
    "        ])\n",
    "        \n",
    "        hidden_states = self.dropout(hidden_states, training=kwargs.get(\"training\", False))\n",
    "        logits = self.qa_outputs(hidden_states)\n",
    "        start_logits, end_logits = tf.split(logits, 2, axis=-1)\n",
    "        start_logits = tf.squeeze(start_logits, axis=-1)\n",
    "        end_logits = tf.squeeze(end_logits, axis=-1)\n",
    "        \n",
    "        return start_logits, end_logits\n",
    "    \n",
    "    \n",
    "def train(model, dataset, loss_fn, optimizer):\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(model, inputs, y_true, loss_fn, optimizer):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(inputs, training=True)\n",
    "            loss  = loss_fn(y_true[0], y_pred[0])\n",
    "            loss += loss_fn(y_true[1], y_pred[1])\n",
    "            scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "    \n",
    "        scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "        gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss, y_pred\n",
    "\n",
    "    epoch_loss = 0.\n",
    "    for batch_num, sample in enumerate(dataset):\n",
    "        loss, y_pred = train_step(\n",
    "            model, sample[:3], sample[4:6], loss_fn, optimizer)\n",
    "\n",
    "        epoch_loss += loss\n",
    "\n",
    "        print(\n",
    "            f\"training ... batch {batch_num+1:03d} : \"\n",
    "            f\"train loss {epoch_loss/(batch_num+1):.3f} \",\n",
    "            end='\\r')\n",
    "        \n",
    "        \n",
    "def predict(model, dataset, loss_fn, optimizer):\n",
    "    \n",
    "    @tf.function\n",
    "    def predict_step(model, inputs):\n",
    "        return model(inputs)\n",
    "        \n",
    "    def to_numpy(*args):\n",
    "        out = []\n",
    "        for arg in args:\n",
    "            if arg.dtype == tf.string:\n",
    "                arg = [s.decode('utf-8') for s in arg.numpy()]\n",
    "                out.append(arg)\n",
    "            else:\n",
    "                arg = arg.numpy()\n",
    "                out.append(arg)\n",
    "        return out\n",
    "    \n",
    "    # Initialize accumulators\n",
    "    offset = tf.zeros([0, 128, 2], dtype=tf.dtypes.int32)\n",
    "    text = tf.zeros([0,], dtype=tf.dtypes.string)\n",
    "    selected_text = tf.zeros([0,], dtype=tf.dtypes.string)\n",
    "    sentiment = tf.zeros([0,], dtype=tf.dtypes.string)\n",
    "    pred_start = tf.zeros([0, 128], dtype=tf.dtypes.float32)\n",
    "    pred_end = tf.zeros([0, 128], dtype=tf.dtypes.float32)\n",
    "    \n",
    "    for batch_num, sample in enumerate(dataset):\n",
    "        \n",
    "        print(f\"predicting ... batch {batch_num+1:03d}\"+\" \"*20, end='\\r')\n",
    "        \n",
    "        y_pred = predict_step(model, sample[:3])\n",
    "        \n",
    "        # add batch to accumulators\n",
    "        pred_start = tf.concat((pred_start, y_pred[0]), axis=0)\n",
    "        pred_end = tf.concat((pred_end, y_pred[1]), axis=0)\n",
    "        offset = tf.concat((offset, sample[3]), axis=0)\n",
    "        text = tf.concat((text, sample[6]), axis=0)\n",
    "        selected_text = tf.concat((selected_text, sample[7]), axis=0)\n",
    "        sentiment = tf.concat((sentiment, sample[8]), axis=0)\n",
    "\n",
    "    # pred_start = tf.nn.softmax(pred_start)\n",
    "    # pred_end = tf.nn.softmax(pred_end)\n",
    "    \n",
    "    pred_start, pred_end, text, selected_text, sentiment, offset = \\\n",
    "        to_numpy(pred_start, pred_end, text, selected_text, sentiment, offset)\n",
    "    \n",
    "    return pred_start, pred_end, text, selected_text, sentiment, offset\n",
    "\n",
    "\n",
    "def decode_prediction(pred_start, pred_end, text, offset, sentiment):\n",
    "    \n",
    "    def decode(pred_start, pred_end, text, offset):\n",
    "\n",
    "        decoded_text = \"\"\n",
    "        for i in range(pred_start, pred_end+1):\n",
    "            decoded_text += text[offset[i][0]:offset[i][1]]\n",
    "            if (i+1) < len(offset) and offset[i][1] < offset[i+1][0]:\n",
    "                decoded_text += \" \"\n",
    "        return decoded_text\n",
    "    \n",
    "    decoded_predictions = []\n",
    "    for i in range(len(text)):\n",
    "        if sentiment[i] == \"neutral\" or len(text[i].split()) < 2:\n",
    "            decoded_text = text[i]\n",
    "        else:\n",
    "            idx_start = np.argmax(pred_start[i])\n",
    "            idx_end = np.argmax(pred_end[i])\n",
    "            if idx_start > idx_end:\n",
    "                idx_end = idx_start \n",
    "            decoded_text = str(decode(idx_start, idx_end, text[i], offset[i]))\n",
    "            if len(decoded_text) == 0:\n",
    "                decoded_text = text[i]\n",
    "        decoded_predictions.append(decoded_text)\n",
    "    \n",
    "    return decoded_predictions\n",
    "\n",
    "def jaccard(str1, str2):\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "III. Run it all: \n",
    "\n",
    "model.create() -> dataset.create() -> train(train) ->\n",
    "       -> predict(val).decode() -> predict(test).decode() -> submit\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold 01\n",
      "\n",
      "epoch 001\n",
      "valid jaccard epoch 001: 0.6923053901943429               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 002\n",
      "valid jaccard epoch 002: 0.6949943268293087               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 003\n",
      "valid jaccard epoch 003: 0.6867768282603347               \n",
      "\n",
      "fold 02\n",
      "\n",
      "epoch 001\n",
      "valid jaccard epoch 001: 0.6926596406167445               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 002\n",
      "valid jaccard epoch 002: 0.6982089976076585               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 003\n",
      "valid jaccard epoch 003: 0.6930363901283845               \n",
      "\n",
      "fold 03\n",
      "\n",
      "epoch 001\n",
      "valid jaccard epoch 001: 0.6887798704089988               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 002\n",
      "valid jaccard epoch 002: 0.6939577184237666               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 003\n",
      "valid jaccard epoch 003: 0.6900999403160519               \n",
      "\n",
      "fold 04\n",
      "\n",
      "epoch 001\n",
      "valid jaccard epoch 001: 0.6926531204213231               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 002\n",
      "valid jaccard epoch 002: 0.6997354888140082               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 003\n",
      "valid jaccard epoch 003: 0.694157903207678               \n",
      "\n",
      "fold 05\n",
      "\n",
      "epoch 001\n",
      "valid jaccard epoch 001: 0.6745946262017565               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 002\n",
      "valid jaccard epoch 002: 0.6767668626104882               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 003\n",
      "valid jaccard epoch 003: 0.6742145488356668               \n"
     ]
    }
   ],
   "source": [
    "num_folds = 5\n",
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "learning_rate = 3e-5\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(\n",
    "    optimizer, 'dynamic')\n",
    "\n",
    "config = BertConfig(output_hidden_states=True, num_labels=2)\n",
    "BertQAModel.DROPOUT_RATE = 0.1\n",
    "BertQAModel.NUM_HIDDEN_STATES = 2\n",
    "model = BertQAModel.from_pretrained(PATH, config=config)\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "kfold = model_selection.KFold(\n",
    "    n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# initialize test predictions\n",
    "test_preds_start = np.zeros((len(test_df), 128), dtype=np.float32)\n",
    "test_preds_end = np.zeros((len(test_df), 128), dtype=np.float32)\n",
    "\n",
    "for fold_num, (train_idx, valid_idx) in enumerate(kfold.split(train_df.text)):\n",
    "    print(\"\\nfold %02d\" % (fold_num+1))\n",
    "        \n",
    "    train_dataset = TweetSentimentDataset.create(\n",
    "        train_df.iloc[train_idx], batch_size, shuffle_buffer_size=2048)\n",
    "    valid_dataset = TweetSentimentDataset.create(\n",
    "        train_df.iloc[valid_idx], batch_size, shuffle_buffer_size=-1)\n",
    "    test_dataset = TweetSentimentDataset.create(\n",
    "        test_df, batch_size, shuffle_buffer_size=-1)\n",
    "    \n",
    "    best_score = float('-inf')\n",
    "    for epoch_num in range(num_epochs):\n",
    "        print(\"\\nepoch %03d\" % (epoch_num+1))\n",
    "        \n",
    "        # train for an epoch\n",
    "        train(model, train_dataset, loss_fn, optimizer)\n",
    "        \n",
    "        # predict validation set and compute jaccardian distances\n",
    "        pred_start, pred_end, text, selected_text, sentiment, offset = \\\n",
    "            predict(model, valid_dataset, loss_fn, optimizer)\n",
    "        \n",
    "        selected_text_pred = decode_prediction(\n",
    "            pred_start, pred_end, text, offset, sentiment)\n",
    "        jaccards = []\n",
    "        for i in range(len(selected_text)):\n",
    "            jaccards.append(\n",
    "                jaccard(selected_text[i], selected_text_pred[i]))\n",
    "        \n",
    "        score = np.mean(jaccards)\n",
    "        print(f\"valid jaccard epoch {epoch_num+1:03d}: {score}\"+\" \"*15)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            # requires you to have 'fold-{fold_num}' folder in PATH:\n",
    "            # model.save_pretrained(PATH+f'fold-{fold_num}')\n",
    "            # or\n",
    "            # model.save_weights(PATH + f'fold-{fold_num}.h5')\n",
    "            \n",
    "            # predict test set\n",
    "            test_pred_start, test_pred_end, test_text, _, test_sentiment, test_offset = \\\n",
    "                predict(model, test_dataset, loss_fn, optimizer)\n",
    "    \n",
    "    # add epoch's best test preds to test preds arrays\n",
    "    test_preds_start += test_pred_start * 0.2\n",
    "    test_preds_end += test_pred_end * 0.2\n",
    "    \n",
    "    # reset model, as well as session and graph (to avoid OOM issues?) \n",
    "    session = tf.compat.v1.get_default_session()\n",
    "    graph = tf.compat.v1.get_default_graph()\n",
    "    del session, graph, model\n",
    "    model = BertQAModel.from_pretrained(PATH, config=config)\n",
    "    \n",
    "# decode test set and add to submission file\n",
    "selected_text_pred = decode_prediction(\n",
    "    test_preds_start, test_preds_end, test_text, test_offset, test_sentiment)\n",
    "\n",
    "\n",
    "# Update 3 (see https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/140942)\n",
    "def f(selected):\n",
    "    return \" \".join(set(selected.lower().split()))\n",
    "submission_df.loc[:, 'selected_text'] = selected_text_pred\n",
    "submission_df['selected_text'] = submission_df['selected_text'].map(f)\n",
    "\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11aa4945ff</td>\n",
       "      <td>wish i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fd1db57dc0</td>\n",
       "      <td>i'm done.haha.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2524332d66</td>\n",
       "      <td>i'm concerned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0fb19285b2</td>\n",
       "      <td>worry. it's hey to need no working guys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e6c9e5e3ab</td>\n",
       "      <td>26th february</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                            selected_text\n",
       "0  11aa4945ff                                   wish i\n",
       "1  fd1db57dc0                           i'm done.haha.\n",
       "2  2524332d66                            i'm concerned\n",
       "3  0fb19285b2  worry. it's hey to need no working guys\n",
       "4  e6c9e5e3ab                            26th february"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
